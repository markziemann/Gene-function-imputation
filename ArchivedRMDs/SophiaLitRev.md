# Overview
The candidate will perform a search of the recent literature (2016-current) on the use of machine learning, deep learning and artificial intelligence technology in inferring genetic networks and gene functions from coexpression and other biological data [1].
The candidate will provide a 2-3 page document summarising what they have found and how it could be applied to generate knowledge from large scale gene expression data such as ARCHS4 and DEE2 [2,3]. 

# Literature Report 

The world of artificial intelligence (AI) is an ever-evolving field, with continuous developments of promising and improved discoveries for more efficient and in-depth technological performance. The emergence of machine learning (ML) and deep learning (DL) have taken AI to the next level, in response to handling larger datasets and non-stop generation of information. This review presents and discusses recent methods of AI, ML and DL concerned with bioinformatics, which can be applied to gene function imputation. 

Here, we first briefly address a few of the more established statistical and ML methods - mainly classification techniques - that have been used for functional prediction. 

**Logistic regression (LR)** is a simple, supervised classification algorithm which maps real-valued inputs into a range from 0 to 1 by a squashing sigmoid function, predicting the probability of a categorical dependent variable (Bonetta & Valentino 2019; Brid 2018). With LR algorithms, caution should be taken when selecting features, as poor model performance may result from including features with either unclear associations or high correlations with the prediction (Ramakrishnaiah , Kuhlmann & Tyagi 2020).
	
A **support vector machine (SVM)** is an algorithm that establishes a maximum-margin hyperplane by maximally separating points that correspond to different classes in a given n-dimensional space (Bonetta & Valentino 2019). This is considered a good option for works involving minimal noise, providing good accuracy and performance. It, however, may not be suitable for problems that are: not linearly separable, involve complex relationships between the target variable and predictors, and involve large numbers of data points (Ramakrishnaiah , Kuhlmann & Tyagi 2020).

**Random forest (RF)** algorithms, used for classification and regression tasks, are aggregations of multiple decision trees which produce a single final result. They address the issues of overfitting and errors due to bias and variance faced by individual decision trees (Liberman 2017). Such models perform well with higher-dimensional data and large numbers of data points. They also have faster running times as a result of each process operating on a data subset and easy parallelisation of the prediction process. However, RFs consume a large amount of memory when working with large complex training datasets (Ramakrishnaiah, Kuhlmann & Tyagi 2020).

Having been used in their separate respective means, these techniques have also been used in conjunction with other methods and frameworks. Such a case is seen in Hakala et al.'s (2019) ensemble system, which combines GO predictions generated by RF and neural network classifiers for protein function prediction by sequence annotation. 

Numerous recent studies have also been conducted specifically for lncRNA function imputation. 

Perron, Provero and Molineris (2017) suggested applying tissue-specific co-expression analysis to lncRNA function annotation. They used gene expression data from different tissues and species to predict lncRNA functions, noting that genes have a higher probability of being functionally correlated when they are co-expressed in more than one species. A **threshold-free algorithm** was created to determine the association strength between a gene and key word using the **"gene set versus ranked list"** approach first seen in the Gene Set Enrichment Analysis (GSEA) algorithm. As a GBA-based study, they strived to project known functional information of better-annotated protein-coding genes to poorly annotated lncRNAs. 
	
Zhang, Zou and Deng (2018) developed their model **BiRWLGO** to conduct large-scale functional annotation of lncRNAs. Also exploiting the GBA principle, functions were assigned to lncRNAs based on known protein annotations. They produced a global network which consisted of a lncRNA similarity network, lncRNA-protein association network, and protein-protein interaction (PPI) network, which were built from interaction and co-expression data. A bi-random walk algorithm was then used to investigate lncRNA-protein similarities, prior to lncRNA annotation with GO terms based on their neighbouring proteins.

Despite the developments, much still lies ahead to reveal the currently very functionally-elusive lncRNAs.
	
Majority of the methods in the literature that has been found and discussed below are recent studies that have been applied to gene and protein function imputation, and have yet to be and show potential for specific implementation for inferring lncRNA functions and networks from namely co-expression and other biological data. 

## Artificial intelligence methods
**Artificial intelligence (AI)** refers to techniques which involve the mimicking of human behaviour by computers (Nguyen et al. 2019).  One branch of AI is **natural language processing (NLP)**. This field allows machines to read, comprehend and extract meaning from the human language (Yse 2019). Despite its focus on speech and text, interesting research has begun on its application to gene studies, with Du et al. (2019) combining this NLP concept with **machine learning** - another branch of AI. This is discussed in the following section. 

## Machine learning methods
**Machine learning (ML)** is a subset of AI which involves computer systems learning from previous experience and improving their task performance accordingly (Nguyen et al. 2019).

As mentioned, Du et al. (2019) combined NLP and ML in their study by applying the concept of word embedding in NLP deep learning research to genes. Their model **Gene2vec** aimed to represent genes as vectors through **gene embedding**, which grouped similar genes into spatial clusters - i.e. mapped similar genes to similar vectors. A distributed representation of genes was produced from transcriptome-wide gene co-expression data, with the embedding matrix solely trained from gene co-expression patterns (with gene co-expression providing the 'context' for gene embedding). This was then able to capture functional gene relationships. Embedded gene factors heavy with gene co-expression pattern information was shown to be useful in tasks such as predicting gene-gene interactions.

Novel methods of gene function imputation can also be created through the integrative effort of multiple techniques. Wekesa, Luan and Meng's (2020) study aimed to identify cell cycle-specific proteins in yeast which are involved with differentially expressed proteins within the PPI network, and to functionally annotate 3538 yeast proteins. Their method **CNPFP** was based on and utilised differential **co-expression analysis** and the **neighbour-voting** (NV) (also known as K-Nearest Neighbours) **algorithm** to **predict protein function** from yeast cell cycle gene expression and PPI data sets, with the degree of similarity between genes measured by biweight midcorrelation (BWM). They described a threefold process: cluster analysis through 'guilt-by-profiling', differential co-expression analysis, and the novel functional annotation of proteins through the neighbour-voting algorithm, with the exploitation of label correlations, genomic features and intrinsic information from the co-expression analysis conducted. Their integrative method was created in recognition that majority of protein function prediction algorithms do not make use of the diverse intrinsic information in feature and label spaces, concluding that the exploitation of intrinsic information from protein relationships improves the quality of predicting functions.
	
Makrodimitris, Reinders and van Ham (2020) presented their **Metric Learning for Co-expression (MLC)** model - a fast algorithm that assigns a GO-term-specific weight to every RNA-seq expression sample. MLC maximised the weighted co-expression of two genes if they were annotated with the same GO term, and minimised this if only one gene in the pair was annotated. Automatic function predictions were constructed from a large amount of expression data to functionally predict genes. Their work was successful in their aim to produce a better weighted co-expression measure than unweighted Pearson correlation for GBA-based function prediction. 

## Deep learning methods
**Deep learning (DL)** refers to a subset of **Neural Networks (NNs)** which makes having computational multi-layer NNs possible (Nguyen et al. 2019). These multi-layer NNs are called **deep neural networks (DNNs)** (Marr n.d.). Such methods have been found to cooperate well with problems involving big data (Bonetta & Valentino 2019), making it a well-suited promising avenue to apply to our study. Such deep learning NNs are capable of independently learning complex and extremely nonlinear relationships, training on large numbers of data points, and scaling up computations. It should be noted, however, that these models are only the biases and weights of the input, hidden, and output nodes - meaning, despite their superiority in predicting coding potential, they provide little help with explaining underlying biological processes through numbers (Ramakrishnaiah, Kuhlmann & Tyagi 2020). 
	
Rifaioglu et al. (2019) have recognised protein function prediction as a multi-label learning problem, as proteins can have more than one functional association (which may also be the case for lncRNA). In response, they created the **DEEPred** model for large-scale protein function prediction. As a **hierarchical stack of multi-task feed-forward DNNs**, their algorithm predicted associations between GO terms and protein sequence data. Given this, it may also have the potential to be developed to predict associations between GO terms and co-expression data for lncRNA. As a multi-task DNN algorithm, it hierarchically constructs complex features from raw input data at each layer, allowing for the extraction of relationships between multiple classes. Predictive performance of multi-task DNNs are also improved by the enhancement of classes with low training sample numbers through shared hidden units among different classes. 
	
**Graph Neural Networks (GNNs)** are a class of DNNs, where rather than learning from structured and tabulated data such as matrices and vectors like other NNs, they learn from unstructured and graph data. Relationships between nodes are graphically modelled, and a numeric representation of the graph is generated (Karagiannakos 2020). This method of graph representation learning follows a neighbourhood aggregation scheme (Xu et al. 2018). GNNs have no requirement of a fixed maximum number of inputs and are invariant towards the order of vehicles in the environment. They use relational information available in the graph, and is provided explicitly due to relation bias, making the implicit inference of relations through the use of collected experiences unnecessary. GNNs can also split intrinsic and extrinsic information, propagate information throughout the network, and gather higher-degree information (Hart & Knoll 2020). 
	
Ioannidis, Marques & Giannakis' (2019) study introduced their **Graph Residual Neural Network (GRNN)** architecture - a graph-based semi-supervised learning (SSL) method. They recognised the inadequacy in representing the relatedness between nodal variables by a single graph, which was the observation in many works involving SSL over graphs. Hence, it is understandable that a single graph representation may fail in situations where nodes are involved in numerous relation types. Just as how people have various relations such as familial, friendship, and professional bonds, this study showed the transition from single-relational to multi-relational graphs, and aimed to impute protein functions over multi-relational protein-to-protein networks. Their GRNN model was used to develop multi-relational graphs and functionally annotate proteins, predicting unknown labels by mapping each node to a corresponding label, using the input feature matrix X. PPI networks related two proteins via multiple cell-dependent relations, and protein classification predicted unknown functions of some proteins based on the functions of a small subset of them. With a known target function on a protein subset, known functions of proteins in X, and multi-relational protein networks, the idea was to predict whether an association existed between proteins in the unlabelled set and the target function. It should be noted that association/connectivity graphs can appear different, depending on the cell type, which is an important fact to consider when modelling and predicting PPIs and their functions.
	
**Convolutional Neural Networks (CNNs)** are another class of DNNs made up of filters called kernels, which perform convolution operations to extract relevant features from the input (Pai 2020). They are specially assigned to problems involving image data, but their ability to also perform well with sequential inputs has begun the consideration of them as recurrent neural network (RNN) alternatives (Nguyen et al. 2019; Pai 2020). CNNs automatically learn filters which derive the correct and relevant features from input data without explicit mentions. They also accurately identify objects, their location and their relation with other objects within an image by capturing the image's spatial features (Pai 2020). CNNs involve parameter sharing and local connectivity, which assists in reducing a system's parameter number and increases computational efficiency (Pokharna 2016).               
	
Yuan & Bar-Joseph (2019) introduced a **convolutional neural network for co-expression (CNNC)**. Their supervised framework for gene relationship inference learned to distinguish between interacting, negative pairs, causal pairs, or any other gene relationship types that could be defined. Each gene pair was represented as an image (histogram), with the CNN used to infer relationships between different expression levels encoded in the image. CNNC was applied to a large amount of single-cell RNA-sequencing (scRNA-seq) data, as well as bulk RNA-seq data, to conduct various tasks, including interaction inference, causality inference, and functional assignment. The model can be trained with any expression dataset, with better performance when provided more data.  In testing CNNC for functional assignments, the method was found to significantly outperform both DNN and GBA. Being supervised allowed the CNN to identify subtle differences between negative and positive pairs, as well as polish the scoring function depending on the application. It addressed the issues of overfitting and false positives faced by previous methods, as well as false negatives experienced in other specific methods caused by assumptions. This method has been deemed easy to use with either general or condition-specific data, and can be applied in many ways -  whether it be extended to integrate complementary data, as a pre-processing procedure, or part of a more advanced network reconstruction method. 
	
**Collaborative filtering (CF)** is a technique used by recommendation systems, which produces a list of content recommendations based on similarities in user interactions. Large amounts of data on a user's behaviours, preferences or activities are collected, analysed and compared to other users to determine similarities with others and hence predict what they will enjoy (Thandapani 2019). While CF methods are known to be used for providing recommendations for users of services such as Netflix and YouTube, they could potentially be applied to bioinformatics for gene function imputation; where RNA-seq data on genes can be analysed and compared to that of other genes to find similarities and predict the functions of functionally-unknown genes.

One subclass of CF is **matrix factorisation (MF)**, or matrix decomposition, which simplifies complex matrix operations by reducing high-dimensional data into its constituent low-dimensional structures, allowing work to be done on the decomposed matrices (Brownlee 2018; Stein-O'Brien 2018). Despite the reduction, the decomposed matrix attempts to preserve as much data as possible that was contained in the original matrix (Stein-O'Brien 2018). Besides dimensionality reduction, matrix factorisation is also commonly used for clustering (Chen & Varshney 2019). Such approaches are hence suitable for high dimensional data, such as that in scRNA-seq data analysis (Stein-O'Brien 2018).  

Peng et al. (2019) presented **PONMF** - their novel method based on **non-negative matrix factorisation (NMF)** for protein function prediction, regularised by PPI and GO similarity networks. Applied to yeast and humans, it successfully integrated diverse biological information and subdivided proteins into different modules, inferring functions of proteins within the same modules. PPI, GO functional similarity (GFS) and protein-GO term association networks were constructed and converted into three matrices prior to the decomposition of the known GO-protein association matrix as a result of constructing the objective function. Protein functions were then predicted from the product of the low-dimensional protein and GO matrices. PPI network datasets were utilised in this study, but the method could potentially be applied to gene expression data. PONMF was shown to outperform the previous NMF-based and propagation-based methods NMFGO and UBiRW, respectively, in protein functional prediction. In creating this framework, Peng et al.'s work recognised the lack of previous methods in addressing the regularisation of the PPI network structure during factorisation, and the notion that proteins tend to share functions with those within the same function modules.
			
**Tensor factorisation** is another method of dimensionality reduction, which aims to produce a compacted representation of the original tensor (Stamile 2019).

Trofimov et al.'s (2020) **factorised embeddings (FE)** model is a self-supervised artificial neural network (ANN) algorithm that simultaneously learns gene and sample representation spaces through tensor factorisation. Ran on RNA-seq data, the learned embedding spaces of the model were shown to capture biologically meaningful information, including that regarding gene-gene co-expression and gene function. The gene representation space was found to be organised with genes grouped by tissue specificity, high correlation and identical GO terms. Despite having focused on protein-coding genes, this method may have the potential to be applied to non-coding genes and lncRNA. 
		
Another deep learning framework which learns embedded representations is KC et al.'s (2019) **gene network embedding (GNE)** method. Robust, scalable and GBA-based, it aimed to predict gene interactions (GI). The study investigated the topological and attribute proximity (similarity between gene expressions) for gene network embedding, constructing a low-dimensional vector representation which preserved both the attribute proximity and topological characteristics of genes. Gene expression data was incorporated with the GI network topological structure data to achieve this. This framework has been deemed usable for various downstream tasks, including gene function prediction (KC et al. 2019).

## References
* Bonetta, R, Valentino, G 2019, 'Machine learning techniques for protein function prediction', Proteins: Structure, Function, and Bioinformatics, vol. 88, no. 3, p. 397, doi:10.1002/prot.25832
* Brid, R 2018, 'Logistic Regression. Brief on Regression analysis', Medium, weblog post, 17 October 2018, retrieved 31 July 2020, <https://medium.com/greyatom/logistic-regression-89e496433063>.
* Brownlee, J 2018, 'A Gentle Introduction to Matrix Factorization for Machine Learning', Machine Learning Mastery, weblog post, 16 February 2018, retrieved 29 July 2020, <https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/>.
* Chen, R & Varshney, LR 2019, 'Optimal Recovery of Missing Values for Non-negative Matrix Factorisation', bioRxiv, Preprint, viewed 29 July 2020, doi: 10.1101/647560
* Du, J, Jia, P, Dai, Y, Tao, C, Zhao, Z, Zhi, D 2019, 'Gene2vec: distributed representation of genes based on co-expression', BMC genomics, vol. 20, no. 82, doi:10.1186/s12864-018-5370-x
* Hakala, K, Kaewphan, S, Bjorne, J, Mehryary, F, Moen, H, Tolvanen, M, Salakoski, T, Ginter, F 2019, 'Neural network and random forest models in protein function prediction, bioRxiv, Preprint, viewed 28 July 2020, doi:10.1101/690271
* Hart, P, Knoll, A 2020, 'Graph Neural Networks and Reinforcement Learning for Behavior Generation in Semantic Environments', arXiv, Preprint, viewed 30 July 2020, <https://deepai.org/publication/graph-neural-networks-and-reinforcement-learning-for-behavior-generation-in-semantic-environments>.
* Ioannidis, VN, Marques, AG, Giannakis, GB 2019, 'Graph Neural Networks for Predicting Protein Functions', Proceedings of the IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), IEEE, Le gosier, Guadeloupe, pp. 221-225, <httsrutps://ieeexplore-ieee-org.ezproxy-b.deakin.edu.au/document/9022646#full-text-section>
* Karagiannakos, S 2020, 'Graph Neural Networks: An overview', The AI Summer, weblog post, 1 February 2020, retrieved 30 July 2020, <https://theaisummer.com/Graph_Neural_Networks/>.
* KC, K, Li, R, Cui, F, Yu, Q, Haake, AR 2019, 'GNE: a deep learning framework for gene network inference by aggregating biological information', BMC Systems Biology, vol. 13, no. 38, doi:10.1186/s12918-019-0694-y
* Liberman, N 2017, 'Decision Trees and Random Forests', towards data science, towards data science, weblog post, 27 January 2017, retrieved 26 July 2020, <https://towardsdatascience.com/decision-trees-and-random-forests-df0c3123f991>.
* Makrodimitris, S, Reinders, MJT & van Ham, RCHJ 2020, 'Metric learning on expression data for gene function prediction', Bioinformatics, vol. 36, no. 4, p. 1182-1190, doi:10.1093/bioinformatics/btz731
* Marr, B n.d., 'Deep Learning Vs Neural Networks - What's The Difference?', Bernard Marr, weblog post, retrieved 30 July 2020, <https://bernardmarr.com/default.asp?contentID=1789>.
* Nguyen, G, Dlugolinsky, S, Bobak, M, Tran, V, Garcia, AL, Heredia, I, Malik, P, Hluchy, L 2019, 'Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey', Artificial Intelligence Review, vol. 52, p. 77-124, doi:10.1007/s10462-018-09679-z 
* Pai, A 2020, 'CNN vs. RNN vs. ANN - Analyzing 3 Types of Neural Networks in Deep Learning', erblog  post, 17 February 2020, retrieved 30 July 2020, <https://www.analyticsvidhya.com/blog/2020/02/cnn-vs-rnn-vs-mlp-analyzing-3-types-of-neural-networks-in-deep-learning/#:~:text=The%20different%20types%20of%20neural,we%20interact%20with%20the%20world.>.
* Peng, W, Li, L, Dai, W, Du, J, Lan, W 2019, 'Predicting protein functions through non-negative matrix factorization regularized by protein-protein interaction network and gene functional information', Proceedings of the 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), IEEE, San Diego, CA, USA, pp. 86-89, <https://ieeexplore-ieee-org.ezproxy-f.deakin.edu.au/document/8983301>.
* Perron, U, Provero, P, Molineris, I 2017, 'In silico prediction of lncRNA function using tissue specific and evolutionary conserved expression', BMC Bioinformatics, vol. 18, no. 144, doi:10.1186/s12859-017-1535-x
* Pokharna, H 2016, 'The best explanation of Convolutional Neural Networks on the Internet!', Medium, weblog post, 29 July 2016, retrieved 30 July 2020, <https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8>.
* Ramakrishnaiah, Y, Kuhlmann, L, Tyagi, S 2020, 'Computational approaches to functionally annotate long noncoding RNA (lncRNA)', Preprint, viewed 30 July 2020, doi:10.20944/preprints202006.0116.v1
* Rifaioglu, AS, Dogan, T, Martin, MJ, Cetin-Atalay, R, Atalay, V 2019, 'Deepred: Automated Protein Function Prediction with Multi-task Feed-forward Deep Neural Networks', vol. 9, no. 7344, doi:10.1038/s41598-019-43708-3
* Stamile, C 2019, 'Tensor Factorisation for Graph Analysis in Python', Medium, weblog post, 28 January 2019, retrieved 29 July 2020, <https://medium.com/yadb/tensor-factorization-for-graph-analysis-in-python-590df44c9f6c>.
* Stein-O'Brien, GL, Arora, R, Culhane, AC, Ochs, MF, Xu, Y, Fertig, EJ 2018, 'Enter the Matrix: Factorisation Uncovers Knowledge from Omics', Trends in Genetics, vol. 34, no. 10, p. 790-805, doi:10.1016/j.tig.2018.07.003
* Thandapani 2019, 'Recommendation Systems: Collaborative Filtering using Matrix Factorization - Simplified', Medium, weblog post, 2 March 2019, retrieved 28 July 2020, <https://medium.com/sfu-cspmp/recommendation-systems-collaborative-filtering-using-matrix-factorization-simplified-2118f4ef2cd3>.
* Trofimov, A, Cohen, JP, Bengio, Y, Perreault, C, Lemieux, S 2020, 'Factorized embeddings learns rich and biologically meaningful embedding spaces using factorised tensor decomposition', Bioinformatics, vol. 36, no. 1, p. i417-i426, doi:10.1093/bioinformatics/btaa488
* Wekesa, JS, Luan, Y, Meng, J 2020, 'Predicting Protein Functions Based on Differential Co-expression and Neighborhood Analysis', Journal of Computational Biology, Preprint, viewed 27 July 2020, doi:10.1089/cmb.2019.0120
* Xu, K, Hu, W, Leskovec, J, Jegelka, S 2018, 'How Powerful are Graph Neural Networks?', Proceedings of the ICLR 2019 Conference on Learning Representations, ICLR, New Orleans, Louisiana, USA, <https://openreview.net/forum?id=ryGs6iA5Km>.
* Yse 2019, 'Your Guide to Natural Language Processing', towards data science, towards data science, weblog post, 16 January 2019, retrieved 28 July 2020, <https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1>.
* Yuan, Y, Bar-Joseph, Z 2019, 'Deep learning for inferring gene relationships from single-cell expression data', PNAS, vol. 116, no. 52, p. 27151-27158, doi:10.1073/pnas.1911536116
* Zhang, J, Zou, S, Deng, L 2018, 'Gene Ontology-based function prediction of long non-coding RNAs using bi-random walk', vol. 11, no. 99, BMC Med Genomics, doi:10.1186/s12920-018-0414-2 



# References

Ballouz S, Verleyen W, Gillis J. Guidance for RNA-seq co-expression network construction and analysis: safety in numbers. Bioinformatics. 2015;31(13):2123‐2130. doi:10.1093/bioinformatics/btv118

Lachmann A, Torre D, Keenan AB, et al. Massive mining of publicly available RNA-seq data from human and mouse. Nat Commun. 2018;9(1):1366. Published 2018 Apr 10. doi:10.1038/s41467-018-03751-6

Ziemann M, Kaspi A, El-Osta A. Digital expression explorer 2: a repository of uniformly processed RNA sequencing data. Gigascience. 2019;8(4):giz022. doi:10.1093/gigascience/giz022
